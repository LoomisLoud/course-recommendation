{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration done on the EPFL recommender system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.model_selection import train_test_split\n",
    "import configparser\n",
    "import mysql.connector as sql\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "# Read the confidential token.\n",
    "credentials = configparser.ConfigParser()\n",
    "credentials.read('credentials.ini')\n",
    "db_connection = sql.connect(host=credentials.get('mysql', 'url'),\n",
    "                            database='semester_project_romain',\n",
    "                            user=credentials.get('mysql', 'username'),\n",
    "                            password=credentials.get('mysql', 'password'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Found courses that should be removed:\n",
    "courses_to_remove = [\n",
    "    \"Admission année sup.\",\n",
    "    \"Projet de master en systèmes de communication\",\n",
    "    \"SHS : Introduction au projet\",\n",
    "    \"Cycle master\",\n",
    "    \"Projet de Master\",\n",
    "    \"Groupe Core courses & options\",\n",
    "    \"Bloc Projets et SHS\",\n",
    "    \"Groupe 2 : Options\",\n",
    "    \"Master SC\",\n",
    "    \"Mineur\",\n",
    "    \"Groupe 1\",\n",
    "    \"Projet en systèmes de communication II\",\n",
    "    \"Cours réservés spécifiquement aux étudiants s'inscrivant pour le mineur Area and Cultural Studies\",\n",
    "    \"SHS : Projet\",\n",
    "    \"Optional project in communication systems\",\n",
    "    \"Mineur : Neurosciences computationnelles\",\n",
    "    \"Stage d'ingénieur crédité avec le PDM (master en Systèmes de communication)\",\n",
    "]\n",
    "\n",
    "domains_to_remove = [\n",
    "    \"Humanities and social sciences\",\n",
    "    \"Programme Sciences humaines et sociales\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# PlanType = \"PLAN_EXAMINE\" ?????\n",
    "all_info = \"\"\"\n",
    "            select distinct \n",
    "                PersonID, \n",
    "                PedagogicalCode, \n",
    "                StudyDomain, \n",
    "                UnitName, \n",
    "                UnitID,\n",
    "                SubjectName, \n",
    "                SubjectID,\n",
    "                SectionName, \n",
    "                YearName,\n",
    "                CourseCode\n",
    "            from \n",
    "                course_enrolments_with_info \n",
    "            where \n",
    "                UnitName like \"%ommunication%\" \n",
    "                and \n",
    "                LevelName = \"Master\"\n",
    "                and (YearName = \"2010-2011\"\n",
    "                or YearName = \"2011-2012\"\n",
    "                or YearName = \"2012-2013\"\n",
    "                or YearName = \"2013-2014\"\n",
    "                or YearName = \"2014-2015\"\n",
    "                or YearName = \"2015-2016\")\n",
    "            \"\"\"\n",
    "all_df = pd.read_sql(all_info, con=db_connection)\n",
    "all_df = all_df[~all_df.SubjectName.isin(courses_to_remove)]\n",
    "# Removing the SHS courses\n",
    "all_df = all_df[~(all_df.StudyDomain.isin(domains_to_remove))]\n",
    "all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "current_courses = \"\"\"\n",
    "            select distinct \n",
    "                PedagogicalCode, \n",
    "                SubjectName, \n",
    "                SubjectID,\n",
    "                StudyDomain,\n",
    "                YearName\n",
    "            from \n",
    "                course_enrolments_with_info \n",
    "            where \n",
    "                UnitName like \"%ommunication%\" \n",
    "                and \n",
    "                LevelName = \"Master\"\n",
    "                and left(PedagogicalCode, 2) = \"MA\"\n",
    "                and YearName = \"2015-2016\"\n",
    "            \"\"\"\n",
    "current_courses_df = pd.read_sql(current_courses, con=db_connection)\n",
    "# These are the current courses (latest data) given in syscom @EPFL\n",
    "current_courses_df = current_courses_df[~current_courses_df.SubjectName.isin(courses_to_remove)]\n",
    "current_courses_df = current_courses_df[~current_courses_df.StudyDomain.isin(domains_to_remove)]\n",
    "current_courses_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most taken courses\n",
    "We need to find a way to get a cleaner dataset of courses, a lot of them are not usefull or outdated and should not be recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Taking a look at the most taken courses\n",
    "registrations_df = all_df.set_index(['SubjectName', 'YearName'])\n",
    "all_df_registrations = registrations_df.groupby(['SubjectName', 'YearName']).size()\n",
    "\n",
    "registrations_df['Registration'] = all_df_registrations\n",
    "registrations_df = registrations_df.reset_index()\n",
    "# Pick only courses that have a study domain (removes bullshit)\n",
    "# such as Projects and groups, minors etc\n",
    "registrations_df = registrations_df[~registrations_df.StudyDomain.isnull()]\n",
    "# Remove the SHS courses\n",
    "registrations_df = registrations_df[~(registrations_df.StudyDomain == \"Programme Sciences humaines et sociales\")]\n",
    "# Removes non important information\n",
    "registrations_df = registrations_df.drop([\n",
    "    'PersonID', \"StudyDomain\", \"SectionName\", \"PedagogicalCode\",\n",
    "    \"CourseCode\"], axis=1)\n",
    "registrations_df = registrations_df.drop_duplicates()\n",
    "registrations_df = registrations_df.set_index(['SubjectName', 'YearName']).sort_index()\n",
    "registrations = registrations_df.sort_values(ascending=False, by='Registration')\n",
    "\n",
    "# Latest data registrations\n",
    "registrations.xs('2015-2016', level='YearName')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the binary matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "courses_matrix = all_df[['PersonID', 'SubjectName']]\n",
    "courses_matrix = courses_matrix.drop_duplicates()\n",
    "courses_matrix = courses_matrix.set_index(['PersonID', 'SubjectName'])\n",
    "\n",
    "def series_to_integers(series):\n",
    "    \"Converts a whole series to integers\"\n",
    "    return pd.to_numeric(series, downcast='integer')\n",
    "\n",
    "# If the course was taken, set it to 1\n",
    "courses_matrix['joined'] = 1\n",
    "courses_matrix = courses_matrix.reset_index().pivot(index='PersonID', columns='SubjectName', values='joined')\n",
    "courses_matrix = courses_matrix.fillna(0)\n",
    "courses_matrix = courses_matrix.apply(series_to_integers)\n",
    "courses_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting test/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Total matrix, use it to split train and test\n",
    "registrations_total = courses_matrix.unstack().reset_index()\n",
    "registrations_total = registrations_total.rename(columns={0: \"Taken\"})\n",
    "#test_set = registrations_total.sample(frac=0.2, replace=False)\n",
    "#train_set = registrations_total - test_set\n",
    "train, test = train_test_split(registrations_total, test_size=0.2)\n",
    "print(\"We have {} rows in total\".format(len(registrations_total)))\n",
    "print(\"Train: {} rows\".format(len(train)))\n",
    "print(\"Test: {} rows\".format(len(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Back to training matrix\n",
    "courses_matrix = train.pivot(index='PersonID', columns='SubjectName', values=\"Taken\")\n",
    "courses_matrix = courses_matrix.fillna(0)\n",
    "courses_matrix = courses_matrix.apply(series_to_integers)\n",
    "courses_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collaborative filtering through correlation matrix\n",
    "We use the Jaccard score to compute the similarity matrix and then apply on the binary matrix to predict good courses to take. \n",
    "\n",
    "#### Collaborative filtering with Jaccard similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import entropy\n",
    "from numpy.linalg import norm\n",
    "import numpy as np\n",
    "\n",
    "def JSD(P, Q):\n",
    "    _P = P / norm(P, ord=1)\n",
    "    _Q = Q / norm(Q, ord=1)\n",
    "    _M = 0.5 * (_P + _Q)\n",
    "    return 0.5 * (entropy(_P, _M) + entropy(_Q, _M))\n",
    "\n",
    "# Using the Jensen-Shannon similarity\n",
    "corr_courses_matrix = pd.DataFrame(np.zeros((courses_matrix.T.shape[0], courses_matrix.T.shape[0])))\n",
    "for i in range(courses_matrix.T.shape[0]):\n",
    "    print(\"Computing similarity: {:.2f}%\".format(100*i/courses_matrix.T.shape[0]), end=\"\\r\")\n",
    "    for j in range(courses_matrix.T.shape[0]):\n",
    "        corr_courses_matrix.iloc[i].iloc[j] = JSD(courses_matrix.T.iloc[i], courses_matrix.T.iloc[j])\n",
    "        \n",
    "corr_courses_matrix = 1 - corr_courses_matrix.replace([np.inf, -np.inf], 1)\n",
    "\n",
    "print(corr_courses_matrix.shape)\n",
    "print(corr_courses_matrix)\n",
    "\n",
    "# Using Jaccard distance\n",
    "#corr_courses_matrix = squareform(1 - pdist(courses_matrix.T, 'jaccard'))\n",
    "\n",
    "# Using Pearson correlation\n",
    "#corr_courses_matrix = np.corrcoef(courses_matrix.T)  \n",
    "\n",
    "#corr_courses_matrix = squareform(pdist(courses_matrix.T, lambda x: JSD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "course_index = courses_matrix.columns\n",
    "\n",
    "def get_course_similarity(course):  \n",
    "    '''Returns correlation vector for a course'''\n",
    "    course_idx = list(course_index).index(course)\n",
    "    return corr_courses_matrix[course_idx]\n",
    "\n",
    "def get_course_recommendations(user_courses):  \n",
    "    '''Given a set of courses, it returns all the courses with their similarity score'''\n",
    "    course_similarities = np.zeros(corr_courses_matrix.shape[0])\n",
    "    for course_id in user_courses:\n",
    "        course_similarities = course_similarities + get_course_similarity(course_id)\n",
    "    similarities_df = pd.DataFrame({\n",
    "        'course_title': course_index,\n",
    "        'sum_similarity': course_similarities\n",
    "        })\n",
    "    similarities_df = similarities_df[similarities_df.course_title.isin(user_courses)]\n",
    "    similarities_df = similarities_df.sort_values(by=['sum_similarity'], ascending=False)\n",
    "    return similarities_df\n",
    "\n",
    "def recommend_row(user_row):\n",
    "    sample_user = 1801481982\n",
    "    sample_user_courses = list(user_row.sort_values(ascending=False).index)\n",
    "    recommendations = get_course_recommendations(sample_user_courses)\n",
    "    \n",
    "# Le'ts try it out for a random user\n",
    "#sample_user = 1801481982\n",
    "sample_user = 1892490156\n",
    "sample_user_courses = list(courses_matrix.loc[sample_user].sort_values(ascending=False).index)\n",
    "recommendations = get_course_recommendations(sample_user_courses)\n",
    "\n",
    "row = courses_matrix.loc[sample_user]\n",
    "user_courses_as_list = list(row[row > 0].index)\n",
    "# We get the top 20 recommended courses\n",
    "print(\"The user {} has the following courses: \\n- {}\\\n",
    "      \\nso we recommend him to pick:\"\n",
    "      .format(sample_user,\n",
    "              \"\\n- \".join(user_courses_as_list)))\n",
    "# Only accept recommendations from latest data courses (2015-2016)\n",
    "accepted_recommendations = recommendations[recommendations.course_title.isin(current_courses_df.SubjectName)]\n",
    "# Removing bloat courses and courses that the user took\n",
    "accepted_recommendations = accepted_recommendations.dropna()\n",
    "accepted_recommendations = accepted_recommendations[~accepted_recommendations.course_title.isin(user_courses_as_list)]\n",
    "\n",
    "# Normalizing the results by dividing by the maximum of the summed similarities\n",
    "accepted_recommendations.sum_similarity = accepted_recommendations.sum_similarity / accepted_recommendations.sum_similarity.max()\n",
    "accepted_recommendations\n",
    "\n",
    "# Code used to predict one course:\n",
    "#favoured_course = 'Distributed information systems'\n",
    "#favoured_course_index = list(courses_index).index(favoured_course)\n",
    "#P = corr_courses_matrix[favoured_course_index]\n",
    "\n",
    "# list the courses with a high correlation with the favoured course\n",
    "#print(list(courses_index[(P>0.3) & (P<1.0)])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collaborative filtering with KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying Surprise with KNN\n",
    "from collections import defaultdict\n",
    "from surprise.dataset import Reader\n",
    "from surprise import SVD, KNNBasic, KNNWithMeans, KNNWithZScore, KNNBaseline, Dataset, evaluate, print_perf, accuracy\n",
    "\n",
    "def get_top_n(predictions, n=10):\n",
    "    '''Return the top-N recommendation for each user from a set of predictions.\n",
    "\n",
    "    Args:\n",
    "        predictions(list of Prediction objects): The list of predictions, as\n",
    "            returned by the test method of an algorithm.\n",
    "        n(int): The number of recommendation to output for each user. Default\n",
    "            is 10.\n",
    "\n",
    "    Returns:\n",
    "    A dict where keys are user (raw) ids and values are lists of tuples:\n",
    "        [(raw item id, rating estimation), ...] of size n.\n",
    "    '''\n",
    "\n",
    "    # First map the predictions to each user.\n",
    "    top_n = defaultdict(list)\n",
    "    for uid, iid, true_r, est, _ in predictions:\n",
    "        top_n[uid].append((iid, est))\n",
    "\n",
    "    # Then sort the predictions for each user and retrieve the k highest ones.\n",
    "    for uid, user_ratings in top_n.items():\n",
    "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_n[uid] = user_ratings[:n]\n",
    "\n",
    "    return top_n\n",
    "\n",
    "def k_fold(data, algorithm):\n",
    "    errors = []\n",
    "    for trainset, testset in data.folds():\n",
    "        # shut up while doing it\n",
    "        from IPython.utils import io\n",
    "        with io.capture_output() as captured:\n",
    "            # train and test algorithm.\n",
    "            algorithm.train(trainset)\n",
    "        predictions = algorithm.test(testset)\n",
    "        errors.append(accuracy.rmse(predictions, verbose=False))\n",
    "    return errors\n",
    "\n",
    "# Drop bullshit data\n",
    "used_data = registrations_total\n",
    "\n",
    "# A reader is still needed but only the rating_scale param is requiered.\n",
    "reader = Reader(rating_scale=(0, 1))\n",
    "# The columns must correspond to user id, item id and ratings (in that order).\n",
    "data = Dataset.load_from_df(used_data[['PersonID', 'SubjectName', 'Taken']], reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# how to make it shut up ?\n",
    "# test and train with verbose=False ?\n",
    "def try_knn(KNN_algo, sim_options, data):\n",
    "    print(\"training: 0.00%\", end='\\r')\n",
    "    k_values = np.arange(10, 60)\n",
    "    results = []\n",
    "    for k in k_values:\n",
    "        algo = KNN_algo(k=k, sim_options=sim_options)\n",
    "        errors = k_fold(data, algo)\n",
    "        results.append((k, np.mean(errors)))\n",
    "        print(\"training: {:.2f}%\".format((k + 1 - k_values[0]) / len(k_values) * 100), end='\\r')\n",
    "    print(\"\\ndone.\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sim_options = {'name': 'pearson_baseline',\n",
    "               'user_based': True  # compute  similarities between items\n",
    "               }\n",
    "\n",
    "# K-fold to find the best k using KNNBaseline\n",
    "knn_baseline_results = try_knn(KNNBaseline, sim_options, data)\n",
    "print(\"RMSE for KNN Baseline:\")\n",
    "plt.plot(*zip(*knn_baseline_results))\n",
    "plt.show()\n",
    "\n",
    "best_rmse_tuple = sorted(knn_baseline_results, key=lambda x: x[1])[0]\n",
    "print(\"The best RMSE is: {} for k = {}\".format(best_rmse_tuple[1], best_rmse_tuple[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_options = {'name': 'pearson_baseline',\n",
    "               'user_based': True  # compute  similarities between items\n",
    "               }\n",
    "# Best one yet, after doing a k-fold on all of the other models: k=22\n",
    "algo = KNNBaseline(k=22, sim_options=sim_options)\n",
    "trainset = data.build_full_trainset()\n",
    "algo.train(trainset)\n",
    "testset = trainset.build_testset()\n",
    "predictions = algo.test(testset)\n",
    "\n",
    "top_n = get_top_n(predictions, n=10)\n",
    "\n",
    "# Print the recommended items for each user\n",
    "for uid, user_ratings in top_n.items():\n",
    "    print(uid, [iid for (iid, _) in user_ratings])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluate performances of our algorithm on the dataset.\n",
    "perf = evaluate(algo, data, measures=['RMSE', 'MAE'])\n",
    "print_perf(perf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Taken courses: \", \"\\n- \".join(list(courses_matrix.loc[946926890][courses_matrix.loc[946926890] == 1].index)))\n",
    "print(\"predictions: \", ['Pattern classification and machine learning', 'TCP/IP networking', 'Mobile networks', 'Mineur : Management, technologie et entrepreneuriat', 'Cryptography and security', np.nan, np.nan, np.nan, np.nan, np.nan])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collaborative filtering with NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying with NMF\n",
    "from surprise import NMF\n",
    "algo = NMF(biased=True, verbose=True)\n",
    "errors = k_fold(data, algo)\n",
    "np.mean(errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collaborative filtering with SlopeOne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying with SlopeOne\n",
    "from surprise import SlopeOne\n",
    "algo = SlopeOne()\n",
    "errors = k_fold(data, algo)\n",
    "np.mean(errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collaborative filtering with CoClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying with co-clustering\n",
    "from surprise import CoClustering\n",
    "algo = CoClustering(n_cltr_u=1, n_cltr_i=8, n_epochs=50, verbose=True)\n",
    "errors = k_fold(data, algo)\n",
    "np.mean(errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collaborative filtering with Collaborative Denoising Auto-Encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Embedding, Flatten, Dropout, merge, Activation\n",
    "from keras.models import Model\n",
    "from keras.regularizers import l2\n",
    "\n",
    "def create(I, U, K, hidden_activation, output_activation, q=0.5, l=0.01):\n",
    "    '''\n",
    "    create model\n",
    "    Reference:\n",
    "      Yao Wu, Christopher DuBois, Alice X. Zheng, Martin Ester.\n",
    "        Collaborative Denoising Auto-Encoders for Top-N Recommender Systems.\n",
    "          The 9th ACM International Conference on Web Search and Data Mining (WSDM'16), p153--162, 2016.\n",
    "\n",
    "    :param I: number of items\n",
    "    :param U: number of users\n",
    "    :param K: number of units in hidden layer\n",
    "    :param hidden_activation: activation function of hidden layer\n",
    "    :param output_activation: activation function of output layer\n",
    "    :param q: drop probability\n",
    "    :param l: regularization parameter of L2 regularization\n",
    "    :return: CDAE\n",
    "    :rtype: keras.models.Model\n",
    "    '''\n",
    "    x_item = Input((I,), name='x_item')\n",
    "    h_item = Dropout(q)(x_item)\n",
    "    h_item = Dense(K, W_regularizer=l2(l), b_regularizer=l2(l))(h_item)\n",
    "\n",
    "    # dtype should be int to connect to Embedding layer\n",
    "    x_user = Input((1,), dtype='int32', name='x_user')\n",
    "    h_user = Embedding(input_dim=U, output_dim=K, input_length=1, W_regularizer=l2(l))(x_user)\n",
    "    h_user = Flatten()(h_user)\n",
    "\n",
    "    h = merge([h_item, h_user], mode='sum')\n",
    "    if hidden_activation:\n",
    "        h = Activation(hidden_activation)(h)\n",
    "    y = Dense(I, activation=output_activation)(h)\n",
    "\n",
    "    return Model(input=[x_item, x_user], output=y)\n",
    "\n",
    "def success_rate(pred, true):\n",
    "    cnt = 0\n",
    "    for i in range(pred.shape[0]):\n",
    "        t = np.where(true[i] == 1) # true set\n",
    "        ary = np.intersect1d(pred[i], t)\n",
    "        if ary.size > 0:\n",
    "            cnt += 1\n",
    "    return cnt * 100 / pred.shape[0]\n",
    "\n",
    "train_users = np.arange(courses_matrix.shape[0])\n",
    "test_users = np.arange(courses_matrix.shape[0])\n",
    "\n",
    "flatten_matrix = courses_matrix.unstack().reset_index()\n",
    "\n",
    "test_x = flatten_matrix.sample(frac=0.2, replace=False).set_index(['SubjectName', 'PersonID'])\n",
    "test_x = test_x.pivot_table(index='PersonID', columns=\"SubjectName\").fillna(0)\n",
    "train_x = courses_matrix - test_x\n",
    "train_x = train_x.apply(axis=1, func=lambda x: x.astype(int)).as_matrix()\n",
    "test_x = test_x.apply(axis=1, func=lambda x: x.astype(int)).as_matrix()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_users = np.array(train_users, dtype=np.int32).reshape(len(train_users), 1)\n",
    "test_x_users = np.array(test_users, dtype=np.int32).reshape(len(test_users), 1)\n",
    "\n",
    "# model\n",
    "model = create(I=train_x.shape[1], U=len(train_users)+1, K=50,\n",
    "                    hidden_activation='relu', output_activation='sigmoid', q=0.50, l=0.01)\n",
    "model.compile(loss='mean_absolute_error', optimizer='adam')\n",
    "model.summary()\n",
    "\n",
    "# train\n",
    "history = model.fit(x=[train_x, train_x_users], y=train_x,\n",
    "                    batch_size=128, nb_epoch=1000, verbose=1,\n",
    "                    validation_data=[[test_x, test_x_users],\n",
    "                    test_x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "pred = model.predict(x=[train_x, np.array(train_users, dtype=np.int32).reshape(len(train_users), 1)])\n",
    "pred = pred * (train_x == 0) # remove watched items from predictions\n",
    "pred = np.argsort(pred)\n",
    "\n",
    "for n in range(1, 11):\n",
    "    sr = success_rate(pred[:, -n:], test_x)\n",
    "    print(\"Success Rate at {:d}: {:f}\".format(n, sr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove students not pairs, and test on them after training, remove k courses, see which ones pop up. Test on other domains. Try with all data. Compute f1. Plot precision and recall. Papers boi faltings on top k recommendations. Co enrollment matrix, weight probabilities of output by the student's chance of taking a course (obligatory). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "flatten = courses_matrix.unstack().reset_index()\n",
    "flatten = flatten.rename(columns={0:\"Taken\"})\n",
    "random_user = flatten[(flatten.Taken == 1) & (flatten.PersonID == 974912207)]\n",
    "random_user_id = courses_matrix.index.get_loc(key=974912207)\n",
    "random_user_predictions = pred[random_user_id, -15:]\n",
    "predicted_courses = [ courses_matrix.columns[i] for i in random_user_predictions ]\n",
    "\n",
    "# Remove courses that were not given in the last year\n",
    "last_year_courses = list(registrations.xs('2015-2016', level='YearName').index)\n",
    "predicted_courses = [c for c in predicted_courses if c in last_year_courses]\n",
    "\n",
    "print(\"The random user picked the following courses: \\n{} \\nHence we propose the following: {}\"\n",
    "     .format(random_user.SubjectName, predicted_courses[::-1] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying the Netflixprize solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find other ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
