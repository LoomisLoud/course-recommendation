{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration done on the EPFL recommender system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.model_selection import train_test_split\n",
    "import configparser\n",
    "import mysql.connector as sql\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "# Read the confidential token.\n",
    "credentials = configparser.ConfigParser()\n",
    "credentials.read('credentials.ini')\n",
    "db_connection = sql.connect(host=credentials.get('mysql', 'url'),\n",
    "                            database='semester_project_romain',\n",
    "                            user=credentials.get('mysql', 'username'),\n",
    "                            password=credentials.get('mysql', 'password'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# PlanType = \"PLAN_EXAMINE\" ?????\n",
    "all_info = \"\"\"\n",
    "            select distinct \n",
    "                PersonID, \n",
    "                PedagogicalCode, \n",
    "                StudyDomain, \n",
    "                UnitName, \n",
    "                UnitID,\n",
    "                SubjectName, \n",
    "                SubjectID,\n",
    "                SectionName, \n",
    "                YearName,\n",
    "                CourseCode\n",
    "            from \n",
    "                course_enrolments_with_info \n",
    "            where \n",
    "                UnitName like \"%ommunication%\" \n",
    "                and \n",
    "                LevelName = \"Master\"\n",
    "                and (YearName = \"2010-2011\"\n",
    "                or YearName = \"2011-2012\"\n",
    "                or YearName = \"2012-2013\"\n",
    "                or YearName = \"2013-2014\"\n",
    "                or YearName = \"2014-2015\"\n",
    "                or YearName = \"2015-2016\")\n",
    "            \"\"\"\n",
    "all_df = pd.read_sql(all_info, con=db_connection)\n",
    "all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "current_courses = \"\"\"\n",
    "            select distinct \n",
    "                PedagogicalCode, \n",
    "                SubjectName, \n",
    "                SubjectID,\n",
    "                YearName\n",
    "            from \n",
    "                course_enrolments_with_info \n",
    "            where \n",
    "                UnitName like \"%ommunication%\" \n",
    "                and \n",
    "                LevelName = \"Master\"\n",
    "                and left(PedagogicalCode, 2) = \"MA\"\n",
    "                and YearName = \"2015-2016\"\n",
    "            \"\"\"\n",
    "current_courses_df = pd.read_sql(current_courses, con=db_connection)\n",
    "# These are the current courses (latest data) given in syscom @EPFL\n",
    "current_courses_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning up\n",
    "We need to find a way to get a cleaner dataset of courses, a lot of them are not usefull or outdated and should not be recommended.\n",
    "Should we weed them out by hand for now ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Taking a look at the most taken courses\n",
    "registrations_df = all_df.set_index(['SubjectName', 'YearName'])\n",
    "all_df_registrations = registrations_df.groupby(['SubjectName', 'YearName']).size()\n",
    "\n",
    "registrations_df['Registration'] = all_df_registrations\n",
    "registrations_df = registrations_df.reset_index()\n",
    "# Pick only courses that have a study domain (removes bullshit)\n",
    "# such as Projects and groups, minors etc\n",
    "registrations_df = registrations_df[~registrations_df.StudyDomain.isnull()]\n",
    "# Remove the SHS courses\n",
    "registrations_df = registrations_df[~(registrations_df.StudyDomain == \"Programme Sciences humaines et sociales\")]\n",
    "# Removes non important information\n",
    "registrations_df = registrations_df.drop([\n",
    "    'PersonID', \"StudyDomain\", \"SectionName\", \"PedagogicalCode\",\n",
    "    \"CourseCode\"], axis=1)\n",
    "registrations_df = registrations_df.drop_duplicates()\n",
    "registrations_df = registrations_df.set_index(['SubjectName', 'YearName']).sort_index()\n",
    "registrations = registrations_df.sort_values(ascending=False, by='Registration')\n",
    "\n",
    "# Latest data registrations\n",
    "registrations.xs('2015-2016', level='YearName')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Found courses that should be removed:\n",
    "courses_to_remove = [\n",
    "    \"Admission année sup.\",\n",
    "    \"Projet de master en systèmes de communication\",\n",
    "    \"SHS : Introduction au projet\",\n",
    "    \"Cycle master\",\n",
    "    \"Projet de Master\",\n",
    "    \"Groupe Core courses & options\",\n",
    "    \"Bloc Projets et SHS\",\n",
    "    \"Groupe 2 : Options\",\n",
    "    \"Master SC\",\n",
    "    \"Mineur\",\n",
    "    \"Groupe 1\",\n",
    "    \"Projet en systèmes de communication II\",\n",
    "    \"Cours réservés spécifiquement aux étudiants s'inscrivant pour le mineur Area and Cultural Studies\",\n",
    "    \"SHS : Projet\",\n",
    "    \"Optional project in communication systems\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the binary matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "courses_matrix = all_df[['PersonID', 'SubjectName']]\n",
    "courses_matrix = courses_matrix.drop_duplicates()\n",
    "courses_matrix = courses_matrix.set_index(['PersonID', 'SubjectName'])\n",
    "\n",
    "def series_to_integers(series):\n",
    "    \"Converts a whole series to integers\"\n",
    "    return pd.to_numeric(series, downcast='integer')\n",
    "\n",
    "# If the course was taken, set it to 1\n",
    "courses_matrix['joined'] = 1\n",
    "courses_matrix = courses_matrix.reset_index().pivot(index='PersonID', columns='SubjectName', values='joined')\n",
    "courses_matrix = courses_matrix.fillna(0)\n",
    "courses_matrix = courses_matrix.apply(series_to_integers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting test/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Total matrix, use it to split train and test\n",
    "registrations_total = courses_matrix.unstack().reset_index()\n",
    "registrations_total = registrations_total.rename(columns={0: \"Taken\"})\n",
    "#test_set = registrations_total.sample(frac=0.2, replace=False)\n",
    "#train_set = registrations_total - test_set\n",
    "train, test = train_test_split(registrations_total, test_size=0.2)\n",
    "print(\"We have {} rows in total\".format(len(registrations_total)))\n",
    "print(\"Train: {} rows\".format(len(train)))\n",
    "print(\"Test: {} rows\".format(len(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Back to training matrix\n",
    "courses_matrix = train.pivot(index='PersonID', columns='SubjectName', values=\"Taken\")\n",
    "courses_matrix = courses_matrix.fillna(0)\n",
    "courses_matrix = courses_matrix.apply(series_to_integers)\n",
    "courses_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collaborative filtering through correlation matrix\n",
    "We use the Jaccard score to compute the similarity matrix and then apply on the binary matrix to predict good courses to take. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Using Jaccard distance\n",
    "corr_courses_matrix = squareform(1 - pdist(courses_matrix.T, 'jaccard'))\n",
    "\n",
    "# Using Pearson correlation\n",
    "#corr_courses_matrix = np.corrcoef(courses_matrix.T)  \n",
    "\n",
    "course_index = courses_matrix.columns\n",
    "\n",
    "def get_course_similarity(course):  \n",
    "    '''Returns correlation vector for a course'''\n",
    "    course_idx = list(course_index).index(course)\n",
    "    return corr_courses_matrix[course_idx]\n",
    "\n",
    "def get_course_recommendations(user_courses):  \n",
    "    '''Given a set of courses, it returns all the courses with their similarity score'''\n",
    "    course_similarities = np.zeros(corr_courses_matrix.shape[0])\n",
    "    for course_id in user_courses:\n",
    "        course_similarities = course_similarities + get_course_similarity(course_id)\n",
    "    similarities_df = pd.DataFrame({\n",
    "        'course_title': course_index,\n",
    "        'sum_similarity': course_similarities\n",
    "        })\n",
    "    similarities_df = similarities_df[similarities_df.course_title.isin(user_courses)]\n",
    "    similarities_df = similarities_df.sort_values(by=['sum_similarity'], ascending=False)\n",
    "    return similarities_df\n",
    "\n",
    "def recommend_row(user_row):\n",
    "    sample_user = 1801481982\n",
    "    sample_user_courses = list(user_row.sort_values(ascending=False).index)\n",
    "    recommendations = get_course_recommendations(sample_user_courses)\n",
    "    \n",
    "# Le'ts try it out for a random user\n",
    "#sample_user = 1801481982\n",
    "sample_user = 1892490156\n",
    "sample_user_courses = list(courses_matrix.loc[sample_user].sort_values(ascending=False).index)\n",
    "recommendations = get_course_recommendations(sample_user_courses)\n",
    "\n",
    "row = courses_matrix.loc[sample_user]\n",
    "# We get the top 20 recommended courses\n",
    "print(\"The user {} has the following courses: \\n{}\\\n",
    "      \\nso we recommend him to pick:\"\n",
    "      .format(sample_user,\n",
    "              \"\\n- \".join(list(row[row > 0].index))))\n",
    "# Only accept recommendations from latest data courses (2015-2016)\n",
    "accepted_recommendations = recommendations[recommendations.course_title.isin(current_courses_df.SubjectName)]\n",
    "# Removing bloat courses\n",
    "accepted_recommendations = accepted_recommendations[~accepted_recommendations.isin(courses_to_remove)]\n",
    "accepted_recommendations = accepted_recommendations.dropna()\n",
    "\n",
    "# Normalizing the results by dividing by the maximum of the summed similarities\n",
    "accepted_recommendations.sum_similarity = accepted_recommendations.sum_similarity / accepted_recommendations.sum_similarity.max()\n",
    "accepted_recommendations\n",
    "\n",
    "# Code used to predict one course:\n",
    "#favoured_course = 'Distributed information systems'\n",
    "#favoured_course_index = list(courses_index).index(favoured_course)\n",
    "#P = corr_courses_matrix[favoured_course_index]\n",
    "\n",
    "# list the courses with a high correlation with the favoured course\n",
    "#print(list(courses_index[(P>0.3) & (P<1.0)])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying Surprise with KNN\n",
    "from collections import defaultdict\n",
    "from surprise.dataset import Reader\n",
    "from surprise import SVD, KNNBasic, KNNWithMeans, KNNWithZScore, KNNBaseline, Dataset, evaluate, print_perf, accuracy\n",
    "\n",
    "def get_top_n(predictions, n=10):\n",
    "    '''Return the top-N recommendation for each user from a set of predictions.\n",
    "\n",
    "    Args:\n",
    "        predictions(list of Prediction objects): The list of predictions, as\n",
    "            returned by the test method of an algorithm.\n",
    "        n(int): The number of recommendation to output for each user. Default\n",
    "            is 10.\n",
    "\n",
    "    Returns:\n",
    "    A dict where keys are user (raw) ids and values are lists of tuples:\n",
    "        [(raw item id, rating estimation), ...] of size n.\n",
    "    '''\n",
    "\n",
    "    # First map the predictions to each user.\n",
    "    top_n = defaultdict(list)\n",
    "    for uid, iid, true_r, est, _ in predictions:\n",
    "        top_n[uid].append((iid, est))\n",
    "\n",
    "    # Then sort the predictions for each user and retrieve the k highest ones.\n",
    "    for uid, user_ratings in top_n.items():\n",
    "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_n[uid] = user_ratings[:n]\n",
    "\n",
    "    return top_n\n",
    "\n",
    "def k_fold(data, algorithm):\n",
    "    errors = []\n",
    "    for trainset, testset in data.folds():\n",
    "        # shut up while doing it\n",
    "        from IPython.utils import io\n",
    "        with io.capture_output() as captured:\n",
    "            # train and test algorithm.\n",
    "            algorithm.train(trainset)\n",
    "        predictions = algorithm.test(testset)\n",
    "        errors.append(accuracy.rmse(predictions, verbose=False))\n",
    "    return errors\n",
    "\n",
    "# Drop bullshit data\n",
    "used_data = registrations_total\n",
    "used_data.SubjectName = used_data.SubjectName[~used_data.SubjectName.isin(courses_to_remove)]\n",
    "\n",
    "# A reader is still needed but only the rating_scale param is requiered.\n",
    "reader = Reader(rating_scale=(0, 1))\n",
    "# The columns must correspond to user id, item id and ratings (in that order).\n",
    "data = Dataset.load_from_df(used_data[['PersonID', 'SubjectName', 'Taken']], reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# how to make it shut up ?\n",
    "# test and train with verbose=False ?\n",
    "def try_knn(KNN_algo, sim_options, data):\n",
    "    print(\"training: 0.00%\", end='\\r')\n",
    "    k_values = np.arange(10, 60)\n",
    "    results = []\n",
    "    for k in k_values:\n",
    "        algo = KNN_algo(k=k, sim_options=sim_options)\n",
    "        errors = k_fold(data, algo)\n",
    "        results.append((k, np.mean(errors)))\n",
    "        print(\"training: {:.2f}%\".format((k + 1 - k_values[0]) / len(k_values) * 100), end='\\r')\n",
    "    print(\"\\ndone.\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sim_options = {'name': 'pearson_baseline',\n",
    "               'user_based': True  # compute  similarities between items\n",
    "               }\n",
    "\n",
    "knn_basic_results = try_knn(KNNBasic, sim_options, data)\n",
    "print(\"RMSE for KNN Basic:\")\n",
    "plt.plot(*zip(*knn_basic_results))\n",
    "plt.show()\n",
    "\n",
    "best_rmse_tuple = sorted(knn_basic_results, key=lambda x: x[1])[0]\n",
    "print(\"The best RMSE is: {} for k = {}\".format(best_rmse_tuple[1], best_rmse_tuple[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_means_results = try_knn(KNNWithMeans, sim_options, data)\n",
    "print(\"RMSE for KNN with Means:\")\n",
    "plt.plot(*zip(*knn_means_results))\n",
    "plt.show()\n",
    "\n",
    "best_rmse_tuple = sorted(knn_means_results, key=lambda x: x[1])[0]\n",
    "print(\"The best RMSE is: {} for k = {}\".format(best_rmse_tuple[1], best_rmse_tuple[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_baseline_results = try_knn(KNNBaseline, sim_options, data)\n",
    "print(\"RMSE for KNN Baseline:\")\n",
    "plt.plot(*zip(*knn_baseline_results))\n",
    "plt.show()\n",
    "\n",
    "best_rmse_tuple = sorted(knn_baseline_results, key=lambda x: x[1])[0]\n",
    "print(\"The best RMSE is: {} for k = {}\".format(best_rmse_tuple[1], best_rmse_tuple[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_zscore_results = try_knn(KNNWithZScore, sim_options, data)\n",
    "print(\"RMSE for KNN Z Score:\")\n",
    "plt.plot(*zip(*knn_zscore_results))\n",
    "plt.show()\n",
    "\n",
    "best_rmse_tuple = sorted(knn_zscore_results, key=lambda x: x[1])[0]\n",
    "print(\"The best RMSE is: {} for k = {}\".format(best_rmse_tuple[1], best_rmse_tuple[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Best one yet\n",
    "algo = KNNBaseline(k=22, sim_options=sim_options)\n",
    "trainset = data.build_full_trainset()\n",
    "algo.train(trainset)\n",
    "testset = trainset.build_testset()\n",
    "predictions = algo.test(testset)\n",
    "\n",
    "top_n = get_top_n(predictions, n=10)\n",
    "\n",
    "# Print the recommended items for each user\n",
    "for uid, user_ratings in top_n.items():\n",
    "    print(uid, [iid for (iid, _) in user_ratings])\n",
    "    \n",
    "# Evaluate performances of our algorithm on the dataset.\n",
    "#perf = evaluate(algo, data, measures=['RMSE', 'MAE'])\n",
    "#print_perf(perf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Taken courses: \", \"\\n- \".join(list(courses_matrix.loc[946926890][courses_matrix.loc[946926890] == 1].index)))\n",
    "print(\"predictions: \", ['Pattern classification and machine learning', 'TCP/IP networking', 'Mobile networks', 'Mineur : Management, technologie et entrepreneuriat', 'Cryptography and security', np.nan, np.nan, np.nan, np.nan, np.nan])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying with NMF\n",
    "from surprise import NMF\n",
    "algo = NMF(biased=True, verbose=True)\n",
    "errors = k_fold(data, algo)\n",
    "np.mean(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying with SlopeOne\n",
    "from surprise import SlopeOne\n",
    "algo = SlopeOne()\n",
    "errors = k_fold(data, algo)\n",
    "np.mean(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying with co-clustering\n",
    "from surprise import CoClustering\n",
    "algo = CoClustering(n_cltr_u=1, n_cltr_i=8, n_epochs=50, verbose=True)\n",
    "errors = k_fold(data, algo)\n",
    "np.mean(errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# try out the following:\n",
    "Jensen–Shannon divergence\n",
    "\n",
    "nn winner take all\n",
    "\n",
    "self orga maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying the Netflixprize solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find other ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
